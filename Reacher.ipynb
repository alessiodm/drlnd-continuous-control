{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reacher World!\n",
    "\n",
    "This is the report for the Udacity Deep Reinforcement Learning Nanodegree continuous control project. See the `README.md` for a description of the Unity environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm\n",
    "\n",
    "The algorithm used to solve the environment is PPO, as described in the [paper](https://arxiv.org/abs/1707.06347) and the Udacity lectures about clipped surrogate function and actor-critic methods.\n",
    "\n",
    "Other resources consulted for the implementation are the Hugging Face [Deep RL Course](https://huggingface.co/learn/deep-rl-course) and the [CleanRL](https://github.com/vwxyzjn/cleanrl) implementation (with its wonderful [detailed explanation](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)).\n",
    "\n",
    "This implementation is notably different in various aspects and code structure (described below).\n",
    "\n",
    "### Actor and Critic Networks\n",
    "\n",
    "Both actor and critic networks are feed-forward neural networks with three fully-connected layers. The input size is the state size (i.e., `33` for the Reacher environment), and the hidden layer dimension is `32`. All activations are `tanh`, and the output layers are linear (i.e., there is no non-linearity processing the last linear layer output).\n",
    "\n",
    "The critic output size is `1`, which represents the value estimate for the state(s) processed by the network. The actor output size is the action _continous space_ number of dimensions (i.e., `4` for the Reacher environment) and it represents the _mean_ value of the Gaussian distribution used to sample the continous action. Additionally for the actor, we train the standard deviation _Parameter_ for the such Gaussian distribution.\n",
    "\n",
    "The two networks do not share layers.\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "The neural network linear layers initialization turns out to be **critical** in order to be able to learn. In particular, weights are scaled by a factor of `1e-3` (similar to the [DDPG pendulum](https://github.com/udacity/deep-reinforcement-learning/blob/master/ddpg-pendulum/model.py) model). Without such scaling, the agent doesn't learn correctly.\n",
    "\n",
    "It is unclear if state / action / reward normalization would help and generalize, or whether other architectures do not suffer from such initialization sensitivity. See future improvements for some consideration.\n",
    "\n",
    "### PPO Implementation Details\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "The PPO loss as described in the paper is:\n",
    "\n",
    "\\begin{equation}\n",
    "L_t^{CLIP + VF + S}(\\theta) = \\^{\\mathbb{E}_t}[L_t^{CLIP}(\\theta) + c_1 L_t^{VF}(\\theta) + c_2 S[\\pi_\\theta](s_t)]\n",
    "\\end{equation}\n",
    "\n",
    "Which is a combined: policy loss + value loss + entropy loss. It is important to optimize a single loss function when the network share layers and weights.\n",
    "\n",
    "But in this implementation, actor and critic networks are separate, so I opted to optimize the two loss functions $L_t^{CLIP}(\\theta)$ and $L_t^{VF}(\\theta)$ separately (no entropy added), much more similarly to the actor-critic network optimization of the Udacity lesson and examples.\n",
    "\n",
    "Clipping policy updates (and gradient) happens as described in the PPO lesson (and the paper itself).\n",
    "\n",
    "#### Training Loop\n",
    "\n",
    "As wonderfully explained in the [CleanRL implementation details](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/) and the PPO paper itself, the PPO training loop is not based on episodes / max_steps. Instead, we keep collecting _trajectory segments_ and progressively train the agent on those. That enables to train in long-lasting environments as well.\n",
    "\n",
    "The PPO training loop has been structured accordingly. But we exploit the Reacher environment specificities (i.e., constant `1001` steps episode duration for all bots) to be able to \"even out\" the trajectory segments (a.k.a., policy rollout steps) and determine if the environment is solved across `100` episodes.\n",
    "\n",
    "See the `unity_env/README.md` for more details about the Reacher environment, and the code itself for the specifics.\n",
    "\n",
    "Finally, the learning happens in mini-batches on every trajectory segment. Having an appropriate number of mini-batches is also very important for the algorithm to learn well and fast.\n",
    "\n",
    "#### Generalized Advantage Estimation\n",
    "\n",
    "GAE has been used and implemented according to the [paper](https://arxiv.org/abs/1506.02438). We reset the computation if the episode ends mid-way during a trajectory segment.\n",
    "\n",
    "Also, the standard advantage estimation has been implemented: $A_t(s, a) = Q_t(s, a) - V_t(s)$. But training takes notably longer and it is slower to learn, though it keeps learning nonetheless.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Hyperparameters have mostly standard values:\n",
    "TODO: Alessio list them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training runs for 125 steps, after which it is considered solved because the average score for 100 episodes (across bots) is greater than `30.0`.\n",
    "\n",
    "Let's import the necessary module to run the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from reacher import ReacherWorld\n",
    "from ppo import PPO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility plotting function\n",
    "def plot(scores):\n",
    "    \"\"\"Plot scores and their running average.\"\"\"\n",
    "    avgs = pd.Series(scores).rolling(100).mean()\n",
    "    x = np.arange(len(scores))\n",
    "    plt.figure('Episode scores')\n",
    "    plt.plot(x, scores, label='Scores')\n",
    "    plt.plot(x, avgs, 'r', label='Running average')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the Reacher world!\n",
    "reacher_world = ReacherWorld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train an agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = reacher_world.new_agent()\n",
    "scores = reacher_world.train(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvements\n",
    "\n",
    "Here are some ideas for future improvements on this project:\n",
    "\n",
    " * Neural network initialization: figure out if different architectures are not prone to sensitivity to initial weights.\n",
    " * Neural network sharing layers: share layers between actor and critic, and use PPO loss function.\n",
    " * Try this PPO implementation in different environments, and generalize it to accept different networks.\n",
    " * Explore different ways of determining rewards, e.g., include [_curiosity_](https://pathak22.github.io/large-scale-curiosity/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Comparisons and Variations\n",
    "\n",
    "Let's compare what happens with different hyperparameters... it is fun to see the struggle to get this right :) Also, just 50 episodes to not take forever!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Returns Computation vs. GAE\n",
    "\n",
    "With standard return computation we can see that learning is much slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = reacher_world.new_agent()\n",
    "scores = reacher_world.train(agent, max_episodes=50, gae_enabled=False)\n",
    "plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Weights Standard Initialization\n",
    "\n",
    "Not rescaling linear layers weight seems to prevent learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = reacher_world.new_agent(weight_mul=1.0)\n",
    "scores = reacher_world.train(agent, max_episodes=50)\n",
    "plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train A Single Batch\n",
    "\n",
    "Training across the entire segment (instead of mini-batches) incredibly slows down policy learning. This is to be expected b/c we perform a single update to the policy with all the collected data, instead of iterating and performing many smaller stochastic updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = reacher_world.new_agent()\n",
    "scores = reacher_world.train(agent, max_episodes=50, n_mini_batches=1)\n",
    "plot(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
